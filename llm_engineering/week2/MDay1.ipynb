{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "377ca8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8e3854e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"API key not found. Please set the OPENAI_API_KEY environment variable.\")\n",
    "elif not api_key.startswith(\"AIza\"):\n",
    "    raise ValueError(\"Invalid API key format. Please check your OPENAI_API_KEY.\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e21a00e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tell_a_joke = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell a joke for a student on the journey to becoming an expert in LLM Engineering\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4240d914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the budding LLM Engineer spend three days trying to get their model to say \"Hello World\"?\n",
       "\n",
       "Because it kept returning:\n",
       "\n",
       "\"As an advanced conversational AI trained on a vast corpus of internet text, I am unable to physically manifest or directly interact with a 'world' in the human sense. However, I can generate the textual representation of the common introductory phrase 'Hello, world.' Would you like me to proceed with this, or perhaps explore the philosophical implications of AI consciousness in greeting existence?\"\n",
       "\n",
       "...and they just needed it to be concise."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "\n",
    "gemini = OpenAI(base_url=GEMINI_BASE_URL, api_key=api_key)\n",
    "\n",
    "response = gemini.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    messages=tell_a_joke,\n",
    ")\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63dcb80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": \n",
    "        \"You toss 2 coins. One of them is heads. What's the probability the other is tails? Answer with the probability only.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d21f045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's list all possible outcomes when tossing two coins. We can represent them as pairs, where the first letter is the result of the first coin and the second letter is the result of the second coin:\n",
       "1. HH (Heads, Heads)\n",
       "2. HT (Heads, Tails)\n",
       "3. TH (Tails, Heads)\n",
       "4. TT (Tails, Tails)\n",
       "\n",
       "There are 4 equally likely outcomes.\n",
       "\n",
       "Now, let's consider the given condition: \"One of them is heads.\"\n",
       "This usually means \"at least one of the coins is heads.\" Let's identify the outcomes that satisfy this condition:\n",
       "*   HH: Yes, at least one coin is heads.\n",
       "*   HT: Yes, at least one coin is heads.\n",
       "*   TH: Yes, at least one coin is heads.\n",
       "*   TT: No, neither coin is heads.\n",
       "\n",
       "So, the possible outcomes, given that \"one of them is heads,\" are {HH, HT, TH}. This forms our reduced sample space, and each of these 3 outcomes is equally likely.\n",
       "\n",
       "Next, we need to find the probability that \"the other is tails\" within this reduced sample space. Let's examine each outcome:\n",
       "\n",
       "*   **HH**: If one of them is heads (say, the first coin), the other coin (the second coin) is also heads. So, in this case, the other is NOT tails.\n",
       "*   **HT**: If one of them is heads (the first coin), the other coin (the second coin) IS tails. This satisfies the condition.\n",
       "*   **TH**: If one of them is heads (the second coin), the other coin (the first coin) IS tails. This satisfies the condition.\n",
       "\n",
       "So, out of the 3 possible outcomes (HH, HT, TH), 2 of them (HT, TH) result in \"the other\" coin being tails.\n",
       "\n",
       "The probability is the number of favorable outcomes divided by the total number of outcomes in the reduced sample space:\n",
       "Probability = (Number of outcomes where the other is tails) / (Total number of outcomes where at least one is heads)\n",
       "Probability = 2 / 3\n",
       "\n",
       "The final answer is $\\boxed{\\frac{2}{3}}$."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    messages=easy_puzzle,\n",
    "    reasoning_effort=\"medium\"\n",
    ")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4d04b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard = \"\"\"\n",
    "On a bookshelf, two volumes of Pushkin stand side by side: the first and the second.\n",
    "The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick.\n",
    "A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume.\n",
    "What distance did it gnaw through?\n",
    "\"\"\"\n",
    "hard_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": hard}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1afa0e9",
   "metadata": {},
   "source": [
    "## Training vs Inference time scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4769bf68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This is a classic riddle that plays on how we visualize books on a shelf versus their internal structure.\n",
       "\n",
       "Here's the trick:\n",
       "\n",
       "1.  **Book Orientation on a Shelf:** When books are placed side by side on a shelf, their spines typically face outwards (towards you).\n",
       "    *   **Volume 1 (on the left):** Its **front cover** is on the far left. Its **back cover** is on the right, touching Volume 2.\n",
       "    *   **Volume 2 (on the right):** Its **front cover** is on the left, touching Volume 1. Its **back cover** is on the far right.\n",
       "\n",
       "2.  **Worm's Path:**\n",
       "    *   The worm starts \"from the **first page** of the first volume.\" The first page of Volume 1 is just *inside* its front cover. If the worm is gnawing from left to right (from Volume 1 to Volume 2), it's already *past* the front cover and the entire block of pages of Volume 1. It only needs to gnaw through the **back cover of Volume 1** to exit that book and enter the next.\n",
       "    *   The worm ends \"to the **last page** of the second volume.\" The last page of Volume 2 is just *inside* its back cover. When the worm enters Volume 2 from Volume 1, it first encounters Volume 2's **front cover**. After gnawing through that, it immediately reaches the \"last page\" of Volume 2, so it does not gnaw through the pages of Volume 2 itself.\n",
       "\n",
       "Therefore, the worm only gnaws through the two *inner* covers:\n",
       "*   The back cover of Volume 1.\n",
       "*   The front cover of Volume 2.\n",
       "\n",
       "**Calculation:**\n",
       "*   Thickness of one cover = 2 mm\n",
       "*   Distance gnawed = Thickness of back cover (V1) + Thickness of front cover (V2)\n",
       "*   Distance = 2 mm + 2 mm = 4 mm\n",
       "\n",
       "The pages' thickness is irrelevant to this specific path due to the starting and ending points relative to the books' orientation.\n",
       "\n",
       "The distance the worm gnawed through is **4 mm**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash\",  \n",
    "    messages=hard_puzzle,\n",
    "    reasoning_effort=\"high\"\n",
    ")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96c3a318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Ollama is running'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(\"http://localhost:11434/\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c474a885",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": \n",
    "        \"You toss 2 coins. One of them is heads. What's the probability the other is tails? Answer with the probability only.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b38e6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1/2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ollama = OpenAI(base_url=\"http://localhost:11434/v1/\", api_key=\"ollama\")\n",
    "response = ollama.chat.completions.create(model=\"llama3.2:latest\", messages=easy_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d48ea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Describe the color Blue to someone who's never been able to see in 1 sentence\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de40c4",
   "metadata": {},
   "source": [
    "## Routers and Abtraction Layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb51da5",
   "metadata": {},
   "source": [
    "#### Openrouter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f01664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openrouter.chat.completions.create(model=\"z-ai/glm-4.5\", messages=tell_a_joke)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d70148ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-google-genai\n",
      "  Downloading langchain_google_genai-4.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting google-genai<2.0.0,>=1.53.0 (from langchain-google-genai)\n",
      "  Downloading google_genai-1.55.0-py3-none-any.whl.metadata (47 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.1.2 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-google-genai) (1.1.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-google-genai) (2.12.4)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (4.12.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (2.43.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (0.28.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (2.32.5)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (9.1.2)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (4.15.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (1.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (3.11)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (6.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (0.16.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (0.4.58)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (6.0.3)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (0.12.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (2.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (0.6.1)\n",
      "Downloading langchain_google_genai-4.0.0-py3-none-any.whl (63 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading google_genai-1.55.0-py3-none-any.whl (703 kB)\n",
      "   ---------------------------------------- 0.0/703.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/703.4 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 262.1/703.4 kB ? eta -:--:--\n",
      "   ---------------------------- --------- 524.3/703.4 kB 840.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 703.4/703.4 kB 1.0 MB/s  0:00:00\n",
      "Installing collected packages: filetype, google-genai, langchain-google-genai\n",
      "\n",
      "   ---------------------------------------- 0/3 [filetype]\n",
      "   ---------------------------------------- 0/3 [filetype]\n",
      "   ---------------------------------------- 0/3 [filetype]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   -------------------------- ------------- 2/3 [langchain-google-genai]\n",
      "   -------------------------- ------------- 2/3 [langchain-google-genai]\n",
      "   -------------------------- ------------- 2/3 [langchain-google-genai]\n",
      "   ---------------------------------------- 3/3 [langchain-google-genai]\n",
      "\n",
      "Successfully installed filetype-1.2.0 google-genai-1.55.0 langchain-google-genai-4.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3198596",
   "metadata": {},
   "source": [
    "#### And now a first look at the powerful, mighty (and quite heavyweight) LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a722f4b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ChatGoogleGenerativeAIError",
     "evalue": "Error calling model 'gemini-2.5-flash' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 13.474079746s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '13s'}]}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\LLM\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:2958\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   2957\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2958\u001b[39m     response: GenerateContentResponse = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2959\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2960\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2961\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\LLM\\.venv\\Lib\\site-packages\\google\\genai\\models.py:5230\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5229\u001b[39m i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5230\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5231\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_config\u001b[49m\n\u001b[32m   5232\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5234\u001b[39m function_map = _extra_utils.get_function_map(parsed_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\LLM\\.venv\\Lib\\site-packages\\google\\genai\\models.py:4012\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   4010\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m4012\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4013\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   4014\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   4017\u001b[39m     config, \u001b[33m'\u001b[39m\u001b[33mshould_return_http_response\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4018\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\LLM\\.venv\\Lib\\site-packages\\google\\genai\\_api_client.py:1388\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1385\u001b[39m http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1386\u001b[39m     http_method, path, request_dict, http_options\n\u001b[32m   1387\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1388\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1389\u001b[39m response_body = (\n\u001b[32m   1390\u001b[39m     response.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1391\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\LLM\\.venv\\Lib\\site-packages\\google\\genai\\_api_client.py:1222\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m   1221\u001b[39m     retry = tenacity.Retrying(**retry_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retry(\u001b[38;5;28mself\u001b[39m._request_once, http_request, stream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\LLM\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\LLM\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\LLM\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\LLM\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m     \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\LLM\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\LLM\\.venv\\Lib\\site-packages\\google\\genai\\_api_client.py:1201\u001b[39m, in \u001b[36mBaseApiClient._request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m   1194\u001b[39m response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m   1195\u001b[39m     method=http_request.method,\n\u001b[32m   1196\u001b[39m     url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1199\u001b[39m     timeout=http_request.timeout,\n\u001b[32m   1200\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1201\u001b[39m \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m   1203\u001b[39m     response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m   1204\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\LLM\\.venv\\Lib\\site-packages\\google\\genai\\errors.py:121\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m    119\u001b[39m   response_json = response.body_segments[\u001b[32m0\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m, {})\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\LLM\\.venv\\Lib\\site-packages\\google\\genai\\errors.py:146\u001b[39m, in \u001b[36mAPIError.raise_error\u001b[39m\u001b[34m(cls, status_code, response_json, response)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n",
      "\u001b[31mClientError\u001b[39m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 13.474079746s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '13s'}]}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_google_genai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGoogleGenerativeAI\n\u001b[32m      2\u001b[39m llm = ChatGoogleGenerativeAI(model=\u001b[33m\"\u001b[39m\u001b[33mgemini-2.5-flash\u001b[39m\u001b[33m\"\u001b[39m, api_key=api_key)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtell_a_joke\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\LLM\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:2461\u001b[39m, in \u001b[36mChatGoogleGenerativeAI.invoke\u001b[39m\u001b[34m(self, input, config, code_execution, stop, **kwargs)\u001b[39m\n\u001b[32m   2458\u001b[39m         msg = \u001b[33m\"\u001b[39m\u001b[33mTools are already defined.code_execution tool can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be defined\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2459\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m-> \u001b[39m\u001b[32m2461\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\LLM\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:398\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> AIMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    395\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    396\u001b[39m         cast(\n\u001b[32m    397\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    408\u001b[39m         ).message,\n\u001b[32m    409\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\LLM\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1117\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1110\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m     **kwargs: Any,\n\u001b[32m   1115\u001b[39m ) -> LLMResult:\n\u001b[32m   1116\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\LLM\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:927\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    926\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m         )\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    935\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\LLM\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1221\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1219\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1225\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\LLM\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:2962\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   2958\u001b[39m     response: GenerateContentResponse = \u001b[38;5;28mself\u001b[39m.client.models.generate_content(\n\u001b[32m   2959\u001b[39m         **request,\n\u001b[32m   2960\u001b[39m     )\n\u001b[32m   2961\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2962\u001b[39m     \u001b[43m_handle_client_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2964\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\LLM\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:145\u001b[39m, in \u001b[36m_handle_client_error\u001b[39m\u001b[34m(e, request)\u001b[39m\n\u001b[32m    143\u001b[39m model_name = request.get(\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33munknown\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    144\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError calling model \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.status\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m: Error calling model 'gemini-2.5-flash' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 13.474079746s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '13s'}]}}"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", api_key=api_key)\n",
    "response = llm.invoke(tell_a_joke)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568f61cf",
   "metadata": {},
   "source": [
    "#### Finally - my personal fave - the wonderfully lightweight LiteLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2078689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting litellm\n",
      "  Downloading litellm-1.80.10-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: aiohttp>=3.10 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from litellm) (3.13.2)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from litellm) (8.3.1)\n",
      "Collecting fastuuid>=0.13.0 (from litellm)\n",
      "  Downloading fastuuid-0.14.0-cp312-cp312-win_amd64.whl.metadata (1.1 kB)\n",
      "Collecting grpcio<1.68.0,>=1.62.3 (from litellm)\n",
      "  Downloading grpcio-1.67.1-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: httpx>=0.23.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from litellm) (0.28.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from litellm) (8.4.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from litellm) (3.1.6)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from litellm) (4.25.1)\n",
      "Requirement already satisfied: openai>=2.8.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from litellm) (2.11.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from litellm) (2.12.4)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from litellm) (1.2.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from litellm) (0.12.0)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from litellm) (0.22.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from jinja2<4.0.0,>=3.1.2->litellm) (3.0.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.30.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from aiohttp>=3.10->litellm) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from aiohttp>=3.10->litellm) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from aiohttp>=3.10->litellm) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from aiohttp>=3.10->litellm) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from aiohttp>=3.10->litellm) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from aiohttp>=3.10->litellm) (1.22.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp>=3.10->litellm) (3.11)\n",
      "Requirement already satisfied: anyio in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from httpx>=0.23.0->litellm) (4.12.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from httpx>=0.23.0->litellm) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from httpx>=0.23.0->litellm) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.23.0->litellm) (0.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from importlib-metadata>=6.8.0->litellm) (3.23.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from openai>=2.8.0->litellm) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from openai>=2.8.0->litellm) (0.12.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from openai>=2.8.0->litellm) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from openai>=2.8.0->litellm) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from tiktoken>=0.7.0->litellm) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from tiktoken>=0.7.0->litellm) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (2.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from tqdm>4->openai>=2.8.0->litellm) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from tokenizers->litellm) (0.36.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (6.0.3)\n",
      "Downloading litellm-1.80.10-py3-none-any.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.3 MB 8.2 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.6/11.3 MB 5.6 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.1/11.3 MB 6.1 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.5/11.3 MB 6.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.0/11.3 MB 6.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.3/11.3 MB 6.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.7/11.3 MB 6.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.0/11.3 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 6.3 MB/s  0:00:01\n",
      "Downloading grpcio-1.67.1-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 1.0/4.3 MB 7.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 2.4/4.3 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 2.6/4.3 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 2.9/4.3 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 3.1/4.3 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 3.1/4.3 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.4/4.3 MB 2.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.7/4.3 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 4.2/4.3 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 2.0 MB/s  0:00:02\n",
      "Downloading fastuuid-0.14.0-cp312-cp312-win_amd64.whl (156 kB)\n",
      "Installing collected packages: grpcio, fastuuid, litellm\n",
      "\n",
      "  Attempting uninstall: grpcio\n",
      "\n",
      "    Found existing installation: grpcio 1.76.0\n",
      "\n",
      "    Uninstalling grpcio-1.76.0:\n",
      "\n",
      "      Successfully uninstalled grpcio-1.76.0\n",
      "\n",
      "   ---------------------------------------- 0/3 [grpcio]\n",
      "   ---------------------------------------- 0/3 [grpcio]\n",
      "   ---------------------------------------- 0/3 [grpcio]\n",
      "   ---------------------------------------- 0/3 [grpcio]\n",
      "   ---------------------------------------- 0/3 [grpcio]\n",
      "   ---------------------------------------- 0/3 [grpcio]\n",
      "   ---------------------------------------- 0/3 [grpcio]\n",
      "   ---------------------------------------- 0/3 [grpcio]\n",
      "   ---------------------------------------- 0/3 [grpcio]\n",
      "   ---------------------------------------- 0/3 [grpcio]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   ---------------------------------------- 3/3 [litellm]\n",
      "\n",
      "Successfully installed fastuuid-0.14.0 grpcio-1.67.1 litellm-1.80.10\n"
     ]
    }
   ],
   "source": [
    "!pip install litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f14d44ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Desktop\\LLM\\.venv\\Lib\\site-packages\\pydantic\\main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 7: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Why did ...er_specific_fields=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Why did the LLM engineer break up with their chatbot?\n",
       "\n",
       "Because after weeks of meticulous prompt engineering, fine-tuning, and RAG implementation, it still confidently told them that a cat is a species of fish and then asked for a raise."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "response = completion(\n",
    "    model=\"gemini/gemini-2.5-flash\",\n",
    "    messages=tell_a_joke\n",
    ")\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70e2c189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 18\n",
      "Output tokens: 1456\n",
      "Total tokens: 1474\n",
      "Total cost: 0.3645 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173ff46d",
   "metadata": {},
   "source": [
    "## Now - let's use LiteLLM to illustrate a Pro-feature: prompt caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f28676d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak, man.\n",
      "  Laer. Where is my father?\n",
      "  King. Dead.\n",
      "  Queen. But not by him!\n",
      "  King. Let him deman\n"
     ]
    }
   ],
   "source": [
    "with open(\"hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hamlet = f.read()\n",
    "\n",
    "loc = hamlet.find(\"Speak, man\")\n",
    "print(hamlet[loc:loc+100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb88d7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = [{\"role\": \"user\", \"content\": \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df439504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Desktop\\LLM\\.venv\\Lib\\site-packages\\pydantic\\main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 7: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='When Lae...er_specific_fields=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "When Laertes asks, \"Where is my father?\" in Hamlet, the reply he receives is:\n",
       "\n",
       "**\"Dead.\"**\n",
       "\n",
       "This is delivered by Gertrude, and it's a stark and devastating announcement that Laertes is not prepared for."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b89f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
