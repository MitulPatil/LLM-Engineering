{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "377ca8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8e3854e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"API key not found. Please set the OPENAI_API_KEY environment variable.\")\n",
    "elif not api_key.startswith(\"AIza\"):\n",
    "    raise ValueError(\"Invalid API key format. Please check your OPENAI_API_KEY.\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e21a00e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tell_a_joke = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell a joke for a student on the journey to becoming an expert in LLM Engineering\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4240d914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the budding LLM Engineer spend three days trying to get their model to say \"Hello World\"?\n",
       "\n",
       "Because it kept returning:\n",
       "\n",
       "\"As an advanced conversational AI trained on a vast corpus of internet text, I am unable to physically manifest or directly interact with a 'world' in the human sense. However, I can generate the textual representation of the common introductory phrase 'Hello, world.' Would you like me to proceed with this, or perhaps explore the philosophical implications of AI consciousness in greeting existence?\"\n",
       "\n",
       "...and they just needed it to be concise."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "\n",
    "gemini = OpenAI(base_url=GEMINI_BASE_URL, api_key=api_key)\n",
    "\n",
    "response = gemini.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    messages=tell_a_joke,\n",
    ")\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63dcb80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": \n",
    "        \"You toss 2 coins. One of them is heads. What's the probability the other is tails? Answer with the probability only.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d21f045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's list all possible outcomes when tossing two coins. We can represent them as pairs, where the first letter is the result of the first coin and the second letter is the result of the second coin:\n",
       "1. HH (Heads, Heads)\n",
       "2. HT (Heads, Tails)\n",
       "3. TH (Tails, Heads)\n",
       "4. TT (Tails, Tails)\n",
       "\n",
       "There are 4 equally likely outcomes.\n",
       "\n",
       "Now, let's consider the given condition: \"One of them is heads.\"\n",
       "This usually means \"at least one of the coins is heads.\" Let's identify the outcomes that satisfy this condition:\n",
       "*   HH: Yes, at least one coin is heads.\n",
       "*   HT: Yes, at least one coin is heads.\n",
       "*   TH: Yes, at least one coin is heads.\n",
       "*   TT: No, neither coin is heads.\n",
       "\n",
       "So, the possible outcomes, given that \"one of them is heads,\" are {HH, HT, TH}. This forms our reduced sample space, and each of these 3 outcomes is equally likely.\n",
       "\n",
       "Next, we need to find the probability that \"the other is tails\" within this reduced sample space. Let's examine each outcome:\n",
       "\n",
       "*   **HH**: If one of them is heads (say, the first coin), the other coin (the second coin) is also heads. So, in this case, the other is NOT tails.\n",
       "*   **HT**: If one of them is heads (the first coin), the other coin (the second coin) IS tails. This satisfies the condition.\n",
       "*   **TH**: If one of them is heads (the second coin), the other coin (the first coin) IS tails. This satisfies the condition.\n",
       "\n",
       "So, out of the 3 possible outcomes (HH, HT, TH), 2 of them (HT, TH) result in \"the other\" coin being tails.\n",
       "\n",
       "The probability is the number of favorable outcomes divided by the total number of outcomes in the reduced sample space:\n",
       "Probability = (Number of outcomes where the other is tails) / (Total number of outcomes where at least one is heads)\n",
       "Probability = 2 / 3\n",
       "\n",
       "The final answer is $\\boxed{\\frac{2}{3}}$."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    messages=easy_puzzle,\n",
    "    reasoning_effort=\"medium\"\n",
    ")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4d04b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard = \"\"\"\n",
    "On a bookshelf, two volumes of Pushkin stand side by side: the first and the second.\n",
    "The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick.\n",
    "A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume.\n",
    "What distance did it gnaw through?\n",
    "\"\"\"\n",
    "hard_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": hard}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1afa0e9",
   "metadata": {},
   "source": [
    "## Training vs Inference time scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4769bf68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This is a classic riddle that plays on how we visualize books on a shelf versus their internal structure.\n",
       "\n",
       "Here's the trick:\n",
       "\n",
       "1.  **Book Orientation on a Shelf:** When books are placed side by side on a shelf, their spines typically face outwards (towards you).\n",
       "    *   **Volume 1 (on the left):** Its **front cover** is on the far left. Its **back cover** is on the right, touching Volume 2.\n",
       "    *   **Volume 2 (on the right):** Its **front cover** is on the left, touching Volume 1. Its **back cover** is on the far right.\n",
       "\n",
       "2.  **Worm's Path:**\n",
       "    *   The worm starts \"from the **first page** of the first volume.\" The first page of Volume 1 is just *inside* its front cover. If the worm is gnawing from left to right (from Volume 1 to Volume 2), it's already *past* the front cover and the entire block of pages of Volume 1. It only needs to gnaw through the **back cover of Volume 1** to exit that book and enter the next.\n",
       "    *   The worm ends \"to the **last page** of the second volume.\" The last page of Volume 2 is just *inside* its back cover. When the worm enters Volume 2 from Volume 1, it first encounters Volume 2's **front cover**. After gnawing through that, it immediately reaches the \"last page\" of Volume 2, so it does not gnaw through the pages of Volume 2 itself.\n",
       "\n",
       "Therefore, the worm only gnaws through the two *inner* covers:\n",
       "*   The back cover of Volume 1.\n",
       "*   The front cover of Volume 2.\n",
       "\n",
       "**Calculation:**\n",
       "*   Thickness of one cover = 2 mm\n",
       "*   Distance gnawed = Thickness of back cover (V1) + Thickness of front cover (V2)\n",
       "*   Distance = 2 mm + 2 mm = 4 mm\n",
       "\n",
       "The pages' thickness is irrelevant to this specific path due to the starting and ending points relative to the books' orientation.\n",
       "\n",
       "The distance the worm gnawed through is **4 mm**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash\",  \n",
    "    messages=hard_puzzle,\n",
    "    reasoning_effort=\"high\"\n",
    ")\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96c3a318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Ollama is running'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(\"http://localhost:11434/\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c474a885",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": \n",
    "        \"You toss 2 coins. One of them is heads. What's the probability the other is tails? Answer with the probability only.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b38e6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1/2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ollama = OpenAI(base_url=\"http://localhost:11434/v1/\", api_key=\"ollama\")\n",
    "response = ollama.chat.completions.create(model=\"llama3.2:latest\", messages=easy_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d48ea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Describe the color Blue to someone who's never been able to see in 1 sentence\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de40c4",
   "metadata": {},
   "source": [
    "## Routers and Abtraction Layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb51da5",
   "metadata": {},
   "source": [
    "#### Openrouter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f01664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openrouter.chat.completions.create(model=\"z-ai/glm-4.5\", messages=tell_a_joke)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d70148ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-google-genai\n",
      "  Downloading langchain_google_genai-4.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting google-genai<2.0.0,>=1.53.0 (from langchain-google-genai)\n",
      "  Downloading google_genai-1.55.0-py3-none-any.whl.metadata (47 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.1.2 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-google-genai) (1.1.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-google-genai) (2.12.4)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (4.12.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (2.43.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (0.28.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (2.32.5)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (9.1.2)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (4.15.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (1.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (3.11)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (6.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (0.16.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (0.4.58)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (6.0.3)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (0.12.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain-google-genai) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (2.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (0.6.1)\n",
      "Downloading langchain_google_genai-4.0.0-py3-none-any.whl (63 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading google_genai-1.55.0-py3-none-any.whl (703 kB)\n",
      "   ---------------------------------------- 0.0/703.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/703.4 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 262.1/703.4 kB ? eta -:--:--\n",
      "   ---------------------------- --------- 524.3/703.4 kB 840.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 703.4/703.4 kB 1.0 MB/s  0:00:00\n",
      "Installing collected packages: filetype, google-genai, langchain-google-genai\n",
      "\n",
      "   ---------------------------------------- 0/3 [filetype]\n",
      "   ---------------------------------------- 0/3 [filetype]\n",
      "   ---------------------------------------- 0/3 [filetype]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   ------------- -------------------------- 1/3 [google-genai]\n",
      "   -------------------------- ------------- 2/3 [langchain-google-genai]\n",
      "   -------------------------- ------------- 2/3 [langchain-google-genai]\n",
      "   -------------------------- ------------- 2/3 [langchain-google-genai]\n",
      "   ---------------------------------------- 3/3 [langchain-google-genai]\n",
      "\n",
      "Successfully installed filetype-1.2.0 google-genai-1.55.0 langchain-google-genai-4.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3198596",
   "metadata": {},
   "source": [
    "#### And now a first look at the powerful, mighty (and quite heavyweight) LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a722f4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='An aspiring LLM Engineer is trying to get their model to generate a simple, accurate list of ingredients for a peanut butter and jelly sandwich.\\n\\nThey prompt: \"List the ingredients for a classic PB&J.\"\\n\\nThe model responds: \"Peanut butter, grape jelly, two slices of artisanal sourdough, a sprinkle of sea salt, and the sweet melancholic memory of childhood summers.\"\\n\\nThe engineer sighs, rubs their temples, and mutters, \"Alright, time to implement a chain-of-thought prompt, define a strict JSON schema, integrate RAG from a certified cookbook, and probably add a sentiment analysis filter... *before* I even think about finetuning for \\'classic\\'!\"' additional_kwargs={} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'} id='lc_run--019b21b6-4eb8-7ae2-831c-5f7e1dbbb69e-0' usage_metadata={'input_tokens': 18, 'output_tokens': 1714, 'total_tokens': 1732, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1572}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", api_key=api_key)\n",
    "response = llm.invoke(tell_a_joke)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568f61cf",
   "metadata": {},
   "source": [
    "#### Finally - my personal fave - the wonderfully lightweight LiteLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2078689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting litellm\n",
      "  Downloading litellm-1.80.10-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: aiohttp>=3.10 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from litellm) (3.13.2)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from litellm) (8.3.1)\n",
      "Collecting fastuuid>=0.13.0 (from litellm)\n",
      "  Downloading fastuuid-0.14.0-cp312-cp312-win_amd64.whl.metadata (1.1 kB)\n",
      "Collecting grpcio<1.68.0,>=1.62.3 (from litellm)\n",
      "  Downloading grpcio-1.67.1-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: httpx>=0.23.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from litellm) (0.28.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from litellm) (8.4.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from litellm) (3.1.6)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from litellm) (4.25.1)\n",
      "Requirement already satisfied: openai>=2.8.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from litellm) (2.11.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from litellm) (2.12.4)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from litellm) (1.2.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from litellm) (0.12.0)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from litellm) (0.22.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from jinja2<4.0.0,>=3.1.2->litellm) (3.0.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.30.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from aiohttp>=3.10->litellm) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from aiohttp>=3.10->litellm) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from aiohttp>=3.10->litellm) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from aiohttp>=3.10->litellm) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from aiohttp>=3.10->litellm) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from aiohttp>=3.10->litellm) (1.22.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp>=3.10->litellm) (3.11)\n",
      "Requirement already satisfied: anyio in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from httpx>=0.23.0->litellm) (4.12.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from httpx>=0.23.0->litellm) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from httpx>=0.23.0->litellm) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.23.0->litellm) (0.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from importlib-metadata>=6.8.0->litellm) (3.23.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from openai>=2.8.0->litellm) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from openai>=2.8.0->litellm) (0.12.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from openai>=2.8.0->litellm) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from openai>=2.8.0->litellm) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from tiktoken>=0.7.0->litellm) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from tiktoken>=0.7.0->litellm) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (2.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from tqdm>4->openai>=2.8.0->litellm) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from tokenizers->litellm) (0.36.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\desktop\\llm\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (6.0.3)\n",
      "Downloading litellm-1.80.10-py3-none-any.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.3 MB 8.2 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.6/11.3 MB 5.6 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.1/11.3 MB 6.1 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.5/11.3 MB 6.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.0/11.3 MB 6.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.3/11.3 MB 6.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.7/11.3 MB 6.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.0/11.3 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 6.3 MB/s  0:00:01\n",
      "Downloading grpcio-1.67.1-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 1.0/4.3 MB 7.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 2.4/4.3 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 2.6/4.3 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 2.9/4.3 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 3.1/4.3 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 3.1/4.3 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.4/4.3 MB 2.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.7/4.3 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 4.2/4.3 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 2.0 MB/s  0:00:02\n",
      "Downloading fastuuid-0.14.0-cp312-cp312-win_amd64.whl (156 kB)\n",
      "Installing collected packages: grpcio, fastuuid, litellm\n",
      "\n",
      "  Attempting uninstall: grpcio\n",
      "\n",
      "    Found existing installation: grpcio 1.76.0\n",
      "\n",
      "    Uninstalling grpcio-1.76.0:\n",
      "\n",
      "      Successfully uninstalled grpcio-1.76.0\n",
      "\n",
      "   ---------------------------------------- 0/3 [grpcio]\n",
      "   ---------------------------------------- 0/3 [grpcio]\n",
      "   ---------------------------------------- 0/3 [grpcio]\n",
      "   ---------------------------------------- 0/3 [grpcio]\n",
      "   ---------------------------------------- 0/3 [grpcio]\n",
      "   ---------------------------------------- 0/3 [grpcio]\n",
      "   ---------------------------------------- 0/3 [grpcio]\n",
      "   ---------------------------------------- 0/3 [grpcio]\n",
      "   ---------------------------------------- 0/3 [grpcio]\n",
      "   ---------------------------------------- 0/3 [grpcio]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   -------------------------- ------------- 2/3 [litellm]\n",
      "   ---------------------------------------- 3/3 [litellm]\n",
      "\n",
      "Successfully installed fastuuid-0.14.0 grpcio-1.67.1 litellm-1.80.10\n"
     ]
    }
   ],
   "source": [
    "!pip install litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f14d44ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Desktop\\LLM\\.venv\\Lib\\site-packages\\pydantic\\main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 7: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Why did ...er_specific_fields=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Why did the LLM engineer break up with their chatbot?\n",
       "\n",
       "Because after weeks of meticulous prompt engineering, fine-tuning, and RAG implementation, it still confidently told them that a cat is a species of fish and then asked for a raise."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "response = completion(\n",
    "    model=\"gemini/gemini-2.5-flash\",\n",
    "    messages=tell_a_joke\n",
    ")\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70e2c189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 18\n",
      "Output tokens: 1456\n",
      "Total tokens: 1474\n",
      "Total cost: 0.3645 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173ff46d",
   "metadata": {},
   "source": [
    "## Now - let's use LiteLLM to illustrate a Pro-feature: prompt caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f28676d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak, man.\n",
      "  Laer. Where is my father?\n",
      "  King. Dead.\n",
      "  Queen. But not by him!\n",
      "  King. Let him deman\n"
     ]
    }
   ],
   "source": [
    "with open(\"hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hamlet = f.read()\n",
    "\n",
    "loc = hamlet.find(\"Speak, man\")\n",
    "print(hamlet[loc:loc+100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb88d7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = [{\"role\": \"user\", \"content\": \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df439504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Desktop\\LLM\\.venv\\Lib\\site-packages\\pydantic\\main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 7: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='When Lae...er_specific_fields=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "When Laertes asks, \"Where is my father?\" in Hamlet, the reply he receives is:\n",
       "\n",
       "**\"Dead.\"**\n",
       "\n",
       "This is delivered by Gertrude, and it's a stark and devastating announcement that Laertes is not prepared for."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b89f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
